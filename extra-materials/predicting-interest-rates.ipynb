{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Modelling lab: Can we predict US government bond interest rates from financial news headlines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we'll combine two datasets, government bond interest rates from the US treasury and business/financial news headlines from various sources, to see if we can predict the changes in interest rates from the news. The question we're getting at is: do news headlines give us an indication of future changes in interest rates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - US treasury data\n",
    "\n",
    "The data for this comes [from the Federal Reserve](https://www.federalreserve.gov/releases/H15/default.htm) and is the file `FRB_H15.csv` in the `data` folder.\n",
    "\n",
    "### 1. Load in the data, making sure excess headers are dealt with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make sure the columns are appropriate types, converting as necessary\n",
    "\n",
    "Are there any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Plot the time series to get a feel for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot the time series for 2014 only\n",
    "\n",
    "The news dataset is from 2014, so let's get a feel for the interest rates in that year only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Add a \"change from yesterday\" column\n",
    "\n",
    "Rather than predicting the interest rate itself we'll try to predict the overnight change in the value from news items on the previous day. For that we'll need a target column, which is the change in interest rate from the day before.\n",
    "\n",
    "Create this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - News\n",
    "\n",
    "Our news headline data comes from the UCI Machine Learning repository ([via Kaggle](https://www.kaggle.com/uciml/news-aggregator-dataset)). The news dataset originally has multiple categories, but the file provided for this exercise is limited to the \"business\" category.\n",
    "\n",
    "### 1. Read in the data\n",
    "\n",
    "The file is zipped to reduce its size, but `pandas` can actually open CSVs if they're in a Gzip archive directly (you don't have to do the unzipping yourself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Do the usual sense checking of the dataset\n",
    "\n",
    "- how many rows are there?\n",
    "- are there missing values?\n",
    "- what are the data types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert the \"timestamp\" column to a date type\n",
    "\n",
    "Have a read of [the data documentation](http://archive.ics.uci.edu/ml/datasets/News+Aggregator) on the UCI ML repository site to see what format the column is currently in (if it's not a format you've come across before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sanity check your date column by looking at what time period it covers (should be dates within 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Merging the datasets\n",
    "\n",
    "Now it's time to combine the two datasets.\n",
    "\n",
    "We want to make sure the dataset accurately reflects the data generating process, so join the news headlines with the interest rates making sure that the date for the headlines is the day *before* the interest rates. We therefore assume that any effect of what's happening in the news takes a day to happen. You can always tweak this assumption later!\n",
    "\n",
    "### 1. Create a column in your news dataset called \"next day\" to store the date + 1 day\n",
    "\n",
    "*Hint: the `datetime` library has a section called `timedelta` that will help!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Merge the two datasets\n",
    "\n",
    "Join the rates data to the news data. Remember to join on the \"next day\" column instead of the original date in your news dataset.\n",
    "\n",
    "After joining ensure the dataset has the same number of rows as your news dataset (to verify the join)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Feature creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Separate your dataset into training and test sets\n",
    "\n",
    "Your training and test features should only contain the titles for now, and your training/test targets are your interest rate changes.\n",
    "\n",
    "We want to do this *before* creating text features, because our vocabulary should come from our **training set only**.\n",
    "\n",
    "Remember, the test set is information you haven't seen yet, so words in those headlines shouldn't count!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create text features using the `TfidfVectorizer`\n",
    "\n",
    "Now we want to create features that represent the TF-IDF scores of the `N` most common (non-stopword) words.\n",
    "\n",
    "Try `N`=100 for now.\n",
    "\n",
    "Use scikit-learn's `TfidfVectorizer` to create a document term matrix of 100 columns, one row per news headline, using the **training set only**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the vocabulary in your `TfidfVectorizer`\n",
    "\n",
    "What are the top 100 words it picked out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transform your test set using your trained vectorizer\n",
    "\n",
    "You should now have a 100-column training set - each column representing a word, and each value representing the TF-IDF score of that word in each title.\n",
    "\n",
    "Use your trained vectorizer object to create a 100-column test set using the set-aside test set titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Rename columns in the training set\n",
    "\n",
    "Use your vectorizer's `vocabulary_` dictionary to get your keys (they need to be sorted alphabetically to correspond to the document-term matrix).\n",
    "\n",
    "Your training dataset should look something like this:\n",
    "\n",
    "| ahead | airlines | american | ... |\n",
    "|---------|---------|------|------|\n",
    "| 0.45 | 0.3 | `NaN` | ... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 - Prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Examine your target (in the training set) - what is the distribution (i.e. what is a credible range for predictions)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compare the cross-validated performance of two different prediction models\n",
    "\n",
    "For this you'll need to:\n",
    "\n",
    "- choose the appropriate type of model (is this classification or regression?)\n",
    "- based on the above, come up with an appropriate metric to measure performance\n",
    "- choose two appropriate algorithms to compare with one another\n",
    "- run both models (with cross-validation) and examine your results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compare your models' feature importance - what words are important to predict interest rate changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Try something to improve one of your models\n",
    "\n",
    "Some ideas:\n",
    "\n",
    "- change the number of features (is more or less than 100 a good idea? This may depend on which words your model thought important)\n",
    "- tune the hyperparameters using grid search\n",
    "- try something more radical and predict the rates themselves, not the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate the best version of your best model on the test set - how did it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
